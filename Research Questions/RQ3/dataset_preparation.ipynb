{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ3: How does the CNN perform on different image tasks and datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 1: Brain MRI Images dataset:**<br>\n",
    "The dataset contains 2 folders: yes and no which contains 253 Brain MRI Images. The folder yes contains 155 Brain MRI Images that are tumorous and the folder no contains 98 Brain MRI Images that are non-tumorous. You can find it [here](https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection). This dataset is then augmented according to the Data Augmentation notebook. After data augmentation, the dataset consists of 1085 positive and 980 examples, resulting in 2065 example images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 2: Brain Tumor MRI dataset:**<br>\n",
    "This dataset contains 7023 images of human brain MRI images which are classified into 4 classes: glioma - meningioma - no tumor and pituitary. No tumor class images were taken from the Br35H dataset. You can find it [here](https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 3: Brain Tumor Classification (MRI) dataset:**<br>\n",
    "The dataset holds 3260 images on T1-weighted contrast-enhanced images that were cleaned and augmented. You can find it [here](https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 4: CE-MRI dataset:**<br>\n",
    "This brain tumor dataset contains 3064 T1-weighted contrast-inhanced images with three kinds of brain tumor. You can find it [here]((https://figshare.com/articles/dataset/brain_tumor_dataset/1512427)).\n",
    "\n",
    "#### **Note**: Dataset 4 has to be downloaded manually from the link above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now proceed with other imports\n",
    "import shutil\n",
    "import cv2\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Brain Tumor MRI (Dataset 2) and Brain Tumor Classification (Dataset 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "  0  148M    0   525    0     0    524      0 82:35:11  0:00:01 82:35:10   524\n",
      "  1  148M    1 1774k    0     0   911k      0  0:02:46  0:00:01  0:02:45 1877k\n",
      "  3  148M    3 5456k    0     0  1853k      0  0:01:22  0:00:02  0:01:20 2807k\n",
      "  6  148M    6 9696k    0     0  2460k      0  0:01:01  0:00:03  0:00:58 3297k\n",
      "  9  148M    9 13.8M    0     0  2876k      0  0:00:52  0:00:04  0:00:48 3607k\n",
      " 11  148M   11 17.4M    0     0  3006k      0  0:00:50  0:00:05  0:00:45 3615k\n",
      " 14  148M   14 21.0M    0     0  3111k      0  0:00:48  0:00:06  0:00:42 3969k\n",
      " 15  148M   15 23.4M    0     0  3024k      0  0:00:50  0:00:07  0:00:43 3714k\n",
      " 17  148M   17 25.4M    0     0  2915k      0  0:00:52  0:00:08  0:00:44 3273k\n",
      " 19  148M   19 29.0M    0     0  2995k      0  0:00:50  0:00:09  0:00:41 3114k\n",
      " 22  148M   22 32.7M    0     0  3060k      0  0:00:49  0:00:10  0:00:39 3125k\n",
      " 23  148M   23 35.6M    0     0  3051k      0  0:00:49  0:00:11  0:00:38 2969k\n",
      " 26  148M   26 39.2M    0     0  3105k      0  0:00:48  0:00:12  0:00:36 3233k\n",
      " 28  148M   28 42.9M    0     0  3138k      0  0:00:48  0:00:14  0:00:34 3532k\n",
      " 31  148M   31 46.7M    0     0  3202k      0  0:00:47  0:00:14  0:00:33 3612k\n",
      " 34  148M   34 50.6M    0     0  3255k      0  0:00:46  0:00:15  0:00:31 3681k\n",
      " 37  148M   37 55.1M    0     0  3329k      0  0:00:45  0:00:16  0:00:29 3994k\n",
      " 39  148M   39 59.0M    0     0  3367k      0  0:00:45  0:00:17  0:00:28 4045k\n",
      " 42  148M   42 63.2M    0     0  3418k      0  0:00:44  0:00:18  0:00:26 4215k\n",
      " 44  148M   44 66.5M    0     0  3411k      0  0:00:44  0:00:19  0:00:25 4031k\n",
      " 47  148M   47 70.4M    0     0  3441k      0  0:00:44  0:00:20  0:00:24 4032k\n",
      " 50  148M   50 74.3M    0     0  3470k      0  0:00:43  0:00:21  0:00:22 3947k\n",
      " 52  148M   52 77.8M    0     0  3471k      0  0:00:43  0:00:22  0:00:21 3845k\n",
      " 55  148M   55 82.6M    0     0  3527k      0  0:00:43  0:00:24  0:00:19 3933k\n",
      " 58  148M   58 87.4M    0     0  3588k      0  0:00:42  0:00:24  0:00:18 4299k\n",
      " 62  148M   62 92.2M    0     0  3642k      0  0:00:41  0:00:25  0:00:16 4485k\n",
      " 65  148M   65 97.1M    0     0  3690k      0  0:00:41  0:00:26  0:00:15 4656k\n",
      " 67  148M   67  101M    0     0  3702k      0  0:00:41  0:00:27  0:00:14 4761k\n",
      " 71  148M   71  105M    0     0  3741k      0  0:00:40  0:00:28  0:00:12 4779k\n",
      " 74  148M   74  110M    0     0  3778k      0  0:00:40  0:00:29  0:00:11 4730k\n",
      " 76  148M   76  114M    0     0  3781k      0  0:00:40  0:00:30  0:00:10 4506k\n",
      " 80  148M   80  119M    0     0  3829k      0  0:00:39  0:00:31  0:00:08 4578k\n",
      " 83  148M   83  124M    0     0  3865k      0  0:00:39  0:00:32  0:00:07 4776k\n",
      " 86  148M   86  128M    0     0  3874k      0  0:00:39  0:00:33  0:00:06 4646k\n",
      " 89  148M   89  132M    0     0  3893k      0  0:00:39  0:00:34  0:00:05 4581k\n",
      " 92  148M   92  136M    0     0  3901k      0  0:00:38  0:00:35  0:00:03 4641k\n",
      " 94  148M   94  140M    0     0  3900k      0  0:00:39  0:00:36  0:00:03 4354k\n",
      " 97  148M   97  144M    0     0  3904k      0  0:00:38  0:00:37  0:00:01 4157k\n",
      "100  148M  100  148M    0     0  3938k      0  0:00:38  0:00:38 --:--:-- 4402k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "  0 86.7M    0  305k    0     0   256k      0  0:05:46  0:00:01  0:05:45  256k\n",
      "  5 86.7M    5 4816k    0     0  2202k      0  0:00:40  0:00:02  0:00:38 4515k\n",
      " 10 86.7M   10 9552k    0     0  2990k      0  0:00:29  0:00:03  0:00:26 4611k\n",
      " 15 86.7M   15 13.7M    0     0  3364k      0  0:00:26  0:00:04  0:00:22 4596k\n",
      " 20 86.7M   20 17.6M    0     0  3479k      0  0:00:25  0:00:05  0:00:20 4436k\n",
      " 23 86.7M   23 20.2M    0     0  3345k      0  0:00:26  0:00:06  0:00:20 4079k\n",
      " 28 86.7M   28 24.5M    0     0  3490k      0  0:00:25  0:00:07  0:00:18 4054k\n",
      " 32 86.7M   32 27.7M    0     0  3465k      0  0:00:25  0:00:08  0:00:17 3767k\n",
      " 35 86.7M   35 30.6M    0     0  3413k      0  0:00:26  0:00:09  0:00:17 3454k\n",
      " 36 86.7M   36 32.0M    0     0  3219k      0  0:00:27  0:00:10  0:00:17 2950k\n",
      " 38 86.7M   38 33.7M    0     0  3085k      0  0:00:28  0:00:11  0:00:17 2763k\n",
      " 40 86.7M   40 35.5M    0     0  2953k      0  0:00:30  0:00:12  0:00:18 2202k\n",
      " 44 86.7M   44 38.7M    0     0  3008k      0  0:00:29  0:00:13  0:00:16 2254k\n",
      " 51 86.7M   51 44.3M    0     0  3201k      0  0:00:27  0:00:14  0:00:13 2811k\n",
      " 57 86.7M   57 49.5M    0     0  3337k      0  0:00:26  0:00:15  0:00:11 3577k\n",
      " 62 86.7M   62 54.6M    0     0  3456k      0  0:00:25  0:00:16  0:00:09 4288k\n",
      " 67 86.7M   67 58.8M    0     0  3506k      0  0:00:25  0:00:17  0:00:08 4907k\n",
      " 74 86.7M   74 64.2M    0     0  3615k      0  0:00:24  0:00:18  0:00:06 5212k\n",
      " 80 86.7M   80 69.4M    0     0  3705k      0  0:00:23  0:00:19  0:00:04 5136k\n",
      " 87 86.7M   87 75.6M    0     0  3837k      0  0:00:23  0:00:20  0:00:03 5356k\n",
      " 93 86.7M   93 81.0M    0     0  3915k      0  0:00:22  0:00:21  0:00:01 5393k\n",
      " 99 86.7M   99 85.9M    0     0  3962k      0  0:00:22  0:00:22 --:--:-- 5525k\n",
      "100 86.7M  100 86.7M    0     0  3976k      0  0:00:22  0:00:22 --:--:-- 5560k\n"
     ]
    }
   ],
   "source": [
    "# Create data directory and download\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/brain-tumor-mri', exist_ok=True)\n",
    "os.makedirs('data/brain-tumor-classification', exist_ok=True)\n",
    "!curl -L -o data/brain-tumor-mri/brain-tumor-mri-dataset.zip -k https://www.kaggle.com/api/v1/datasets/download/masoudnickparvar/brain-tumor-mri-dataset\n",
    "!curl -L -o data/brain-tumor-classification/brain-tumor-classification-dataset.zip -k https://www.kaggle.com/api/v1/datasets/download/sartajbhuvaji/brain-tumor-classification-mri\n",
    "\n",
    "# Unzip the files\n",
    "with zipfile.ZipFile('data/brain-tumor-mri/brain-tumor-mri-dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/brain-tumor-mri/')\n",
    "\n",
    "with zipfile.ZipFile('data/brain-tumor-classification/brain-tumor-classification-dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/brain-tumor-classification/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructure Dataset 2 and 3 to yes/no folders and balance the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing (brain-tumor-classification): Yes: 2764, No: 500\n",
      "After balancing (brain-tumor-classification): Yes: 500, No: 500\n",
      "Before balancing (brain-tumor-mri): Yes: 5023, No: 2000\n",
      "After balancing (brain-tumor-mri): Yes: 2000, No: 2000\n"
     ]
    }
   ],
   "source": [
    "# Define the base source and destination paths\n",
    "base_data_path = \"data\"\n",
    "new_structure_path = \"data_restructured\"\n",
    "\n",
    "# List of datasets to process\n",
    "datasets = [\"brain-tumor-classification\", \"brain-tumor-mri\"]\n",
    "\n",
    "# Define tumor types for each dataset\n",
    "dataset_tumor_types = {\n",
    "    \"brain-tumor-classification\": {\n",
    "        \"tumor_types\": [\"glioma_tumor\", \"meningioma_tumor\", \"pituitary_tumor\"],\n",
    "        \"no_tumor\": \"no_tumor\"\n",
    "    },\n",
    "    \"brain-tumor-mri\": {\n",
    "        \"tumor_types\": [\"glioma\", \"meningioma\", \"pituitary\"],\n",
    "        \"no_tumor\": \"notumor\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Create dataset-specific restructured path\n",
    "    dataset_restructured_path = os.path.join(new_structure_path, dataset)\n",
    "    os.makedirs(dataset_restructured_path, exist_ok=True)\n",
    "    os.makedirs(os.path.join(dataset_restructured_path, \"yes\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dataset_restructured_path, \"no\"), exist_ok=True)\n",
    "\n",
    "    # Get the tumor types and no_tumor name for the current dataset\n",
    "    tumor_types = dataset_tumor_types[dataset][\"tumor_types\"]\n",
    "    notumor_type = dataset_tumor_types[dataset][\"no_tumor\"]\n",
    "\n",
    "    # Copy tumor data into 'yes' (flattened) for the current dataset\n",
    "    for tumor_type in tumor_types:\n",
    "        for dataset_type in [\"Testing\", \"Training\"]:\n",
    "            source_dir = os.path.join(base_data_path, dataset, dataset_type, tumor_type)\n",
    "            if os.path.exists(source_dir):\n",
    "                # Copy all files directly to 'yes'\n",
    "                for item in os.listdir(source_dir):\n",
    "                    source_item = os.path.join(source_dir, item)\n",
    "                    if os.path.isfile(source_item):\n",
    "                        destination_item = os.path.join(dataset_restructured_path, \"yes\", item)\n",
    "                        # Avoid overwriting by adding a number if file exists\n",
    "                        base_name, extension = os.path.splitext(item)\n",
    "                        counter = 1\n",
    "                        while os.path.exists(destination_item):\n",
    "                            new_name = f\"{base_name}_{counter}{extension}\"\n",
    "                            destination_item = os.path.join(dataset_restructured_path, \"yes\", new_name)\n",
    "                            counter += 1\n",
    "                        shutil.copy2(source_item, destination_item)\n",
    "\n",
    "    # Copy notumor data into 'no' (flattened) for the current dataset\n",
    "    for dataset_type in [\"Testing\", \"Training\"]:\n",
    "        source_dir = os.path.join(base_data_path, dataset, dataset_type, notumor_type)\n",
    "        if os.path.exists(source_dir):\n",
    "            # Copy all files directly to 'no'\n",
    "            for item in os.listdir(source_dir):\n",
    "                source_item = os.path.join(source_dir, item)\n",
    "                if os.path.isfile(source_item):\n",
    "                    destination_item = os.path.join(dataset_restructured_path, \"no\", item)\n",
    "                    # Avoid overwriting by adding a number if file exists\n",
    "                    base_name, extension = os.path.splitext(item)\n",
    "                    counter = 1\n",
    "                    while os.path.exists(destination_item):\n",
    "                        new_name = f\"{base_name}_{counter}{extension}\"\n",
    "                        destination_item = os.path.join(dataset_restructured_path, \"no\", new_name)\n",
    "                        counter += 1\n",
    "                    shutil.copy2(source_item, destination_item)\n",
    "\n",
    "    # Balance the dataset\n",
    "    yes_path = os.path.join(dataset_restructured_path, \"yes\")\n",
    "    no_path = os.path.join(dataset_restructured_path, \"no\")\n",
    "\n",
    "    # Count the number of files in each folder for the current dataset\n",
    "    yes_files = [f for f in os.listdir(yes_path) if os.path.isfile(os.path.join(yes_path, f))]\n",
    "    no_files = [f for f in os.listdir(no_path) if os.path.isfile(os.path.join(no_path, f))]\n",
    "\n",
    "    yes_count = len(yes_files)\n",
    "    no_count = len(no_files)\n",
    "\n",
    "    print(f\"Before balancing ({dataset}): Yes: {yes_count}, No: {no_count}\")\n",
    "\n",
    "    # If yes has more samples than no, randomly remove samples from yes\n",
    "    if yes_count > no_count:\n",
    "        # Calculate how many to remove from yes to match no\n",
    "        excess = yes_count - no_count\n",
    "        # Randomly select files to remove\n",
    "        files_to_remove = random.sample(yes_files, excess)\n",
    "        # Remove the selected files\n",
    "        for file in files_to_remove:\n",
    "            os.remove(os.path.join(yes_path, file))\n",
    "\n",
    "    # Recount after balancing\n",
    "    yes_files = [f for f in os.listdir(yes_path) if os.path.isfile(os.path.join(yes_path, f))]\n",
    "    no_files = [f for f in os.listdir(no_path) if os.path.isfile(os.path.join(no_path, f))]\n",
    "\n",
    "    yes_count = len(yes_files)\n",
    "    no_count = len(no_files)\n",
    "\n",
    "    print(f\"After balancing ({dataset}): Yes: {yes_count}, No: {no_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For dataset 4, run the following cell to prepare the data after downloading it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "def process_mat_files(input_base, output_base):\n",
    "    class_names = {1: \"meningioma\", 2: \"glioma\", 3: \"pituitary\"}\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "    for class_name in class_names.values():\n",
    "        os.makedirs(os.path.join(output_base, class_name), exist_ok=True)\n",
    "\n",
    "    curStart = 1\n",
    "    curEnd = 766\n",
    "\n",
    "    mat_files = []\n",
    "\n",
    "    # Process each part\n",
    "    for _ in range(4):\n",
    "        input_folder = os.path.join(input_base, f\"brainTumorDataPublic_{curStart}-{curEnd}\")\n",
    "        mat_files = [f for f in os.listdir(input_folder) if f.endswith('.mat')]\n",
    "        curStart = curEnd + 1\n",
    "        curEnd = min(curEnd + 766, 3064)\n",
    "        \n",
    "        for mat_file in mat_files:\n",
    "                \n",
    "                file_path = os.path.join(input_folder, mat_file)\n",
    "                try:\n",
    "                    with h5py.File(file_path, 'r') as mat_data:\n",
    "                        cjdata = mat_data['cjdata']\n",
    "                        \n",
    "                        im1 = np.array(cjdata['image'], dtype=np.float64).T  # Transpose if needed\n",
    "                        min1, max1 = im1.min(), im1.max()\n",
    "                        im = ((255 / (max1 - min1)) * (im1 - min1)).astype(np.uint8)\n",
    "                        label = int(cjdata['label'][()][0][0])  # Extract label properly\n",
    "\n",
    "                        label_folder = os.path.join(output_base, class_names[label])\n",
    "                        os.makedirs(label_folder, exist_ok=True)\n",
    "\n",
    "                        file_name_base, _ = os.path.splitext(mat_file)\n",
    "                        output_file_path = os.path.join(label_folder, f\"{file_name_base}.jpg\")\n",
    "\n",
    "                        cv2.imwrite(output_file_path, im)  \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
