{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ3: How does the CNN perform on different image tasks and datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 1: Brain MRI Images dataset:**<br>\n",
    "The dataset contains 2 folders: yes and no which contains 253 Brain MRI Images. The folder yes contains 155 Brain MRI Images that are tumorous and the folder no contains 98 Brain MRI Images that are non-tumorous. You can find it [here](https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection). This dataset is then augmented according to the Data Augmentation notebook. After data augmentation, the dataset consists of 1085 positive and 980 examples, resulting in 2065 example images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 2: Brain Tumor MRI dataset:**<br>\n",
    "This dataset contains 7023 images of human brain MRI images which are classified into 4 classes: glioma - meningioma - no tumor and pituitary. No tumor class images were taken from the Br35H dataset. You can find it [here](https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset 3: Brain Tumor Classification (MRI) dataset:**<br>\n",
    "The dataset holds 3260 images on T1-weighted contrast-enhanced images that were cleaned and augmented. You can find it [here](https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now proceed with other imports\n",
    "import shutil\n",
    "import cv2\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Brain Tumor MRI (Dataset 2) and Brain Tumor Classification (Dataset 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  2  148M    2 4205k    0     0  3099k      0  0:00:49  0:00:01  0:00:48 4243k\n",
      "  7  148M    7 11.7M    0     0  5082k      0  0:00:29  0:00:02  0:00:27 6014k\n",
      " 13  148M   13 19.4M    0     0  5931k      0  0:00:25  0:00:03  0:00:22 6658k\n",
      " 17  148M   17 26.4M    0     0  6230k      0  0:00:24  0:00:04  0:00:20 6801k\n",
      " 22  148M   22 33.5M    0     0  6419k      0  0:00:23  0:00:05  0:00:18 6891k\n",
      " 27  148M   27 40.2M    0     0  6489k      0  0:00:23  0:00:06  0:00:17 7407k\n",
      " 31  148M   31 46.8M    0     0  6525k      0  0:00:23  0:00:07  0:00:16 7206k\n",
      " 36  148M   36 54.1M    0     0  6641k      0  0:00:22  0:00:08  0:00:14 7118k\n",
      " 41  148M   41 61.8M    0     0  6769k      0  0:00:22  0:00:09  0:00:13 7238k\n",
      " 46  148M   46 69.4M    0     0  6867k      0  0:00:22  0:00:10  0:00:12 7347k\n",
      " 50  148M   50 75.4M    0     0  6806k      0  0:00:22  0:00:11  0:00:11 7209k\n",
      " 56  148M   56 83.6M    0     0  6934k      0  0:00:21  0:00:12  0:00:09 7536k\n",
      " 61  148M   61 91.6M    0     0  7028k      0  0:00:21  0:00:13  0:00:08 7676k\n",
      " 67  148M   67 99.6M    0     0  7109k      0  0:00:21  0:00:14  0:00:07 7745k\n",
      " 72  148M   72  107M    0     0  7174k      0  0:00:21  0:00:15  0:00:06 7811k\n",
      " 77  148M   77  115M    0     0  7229k      0  0:00:21  0:00:16  0:00:05 8188k\n",
      " 82  148M   82  122M    0     0  7254k      0  0:00:20  0:00:17  0:00:03 8044k\n",
      " 87  148M   87  130M    0     0  7288k      0  0:00:20  0:00:18  0:00:02 7980k\n",
      " 92  148M   92  137M    0     0  7257k      0  0:00:20  0:00:19  0:00:01 7683k\n",
      " 97  148M   97  144M    0     0  7284k      0  0:00:20  0:00:20 --:--:-- 7622k\n",
      "100  148M  100  148M    0     0  7308k      0  0:00:20  0:00:20 --:--:-- 7600k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "\n",
      "  0 86.7M    0  3264    0     0   5992      0  4:13:07 --:--:--  4:13:07  5992\n",
      "  3 86.7M    3 3456k    0     0  2399k      0  0:00:37  0:00:01  0:00:36 3853k\n",
      " 13 86.7M   13 11.3M    0     0  4778k      0  0:00:18  0:00:02  0:00:16 6145k\n",
      " 22 86.7M   22 19.4M    0     0  5787k      0  0:00:15  0:00:03  0:00:12 6874k\n",
      " 31 86.7M   31 27.2M    0     0  6281k      0  0:00:14  0:00:04  0:00:10 7158k\n",
      " 38 86.7M   38 33.6M    0     0  6327k      0  0:00:14  0:00:05  0:00:09 7029k\n",
      " 45 86.7M   45 39.2M    0     0  6242k      0  0:00:14  0:00:06  0:00:08 7350k\n",
      " 52 86.7M   52 45.3M    0     0  6240k      0  0:00:14  0:00:07  0:00:07 6953k\n",
      " 60 86.7M   60 52.4M    0     0  6365k      0  0:00:13  0:00:08  0:00:05 6763k\n",
      " 68 86.7M   68 59.0M    0     0  6406k      0  0:00:13  0:00:09  0:00:04 6517k\n",
      " 75 86.7M   75 65.5M    0     0  6425k      0  0:00:13  0:00:10  0:00:03 6532k\n",
      " 81 86.7M   81 70.7M    0     0  6333k      0  0:00:14  0:00:11  0:00:03 6449k\n",
      " 89 86.7M   89 77.2M    0     0  6353k      0  0:00:13  0:00:12  0:00:01 6521k\n",
      " 95 86.7M   95 82.7M    0     0  6298k      0  0:00:14  0:00:13  0:00:01 6184k\n",
      "100 86.7M  100 86.7M    0     0  6223k      0  0:00:14  0:00:14 --:--:-- 5868k\n"
     ]
    }
   ],
   "source": [
    "# Create data directory and download\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/brain-tumor-mri', exist_ok=True)\n",
    "os.makedirs('data/brain-tumor-classification', exist_ok=True)\n",
    "!curl -L -o data/brain-tumor-mri/brain-tumor-mri-dataset.zip -k https://www.kaggle.com/api/v1/datasets/download/masoudnickparvar/brain-tumor-mri-dataset\n",
    "!curl -L -o data/brain-tumor-classification/brain-tumor-classification-dataset.zip -k https://www.kaggle.com/api/v1/datasets/download/sartajbhuvaji/brain-tumor-classification-mri\n",
    "\n",
    "# Unzip the files\n",
    "with zipfile.ZipFile('data/brain-tumor-mri/brain-tumor-mri-dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/brain-tumor-mri/')\n",
    "\n",
    "with zipfile.ZipFile('data/brain-tumor-classification/brain-tumor-classification-dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/brain-tumor-classification/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructure Dataset 2 and 3 to yes/no folders and balance the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing (brain-tumor-classification): Yes: 2764, No: 500\n",
      "After balancing (brain-tumor-classification): Yes: 500, No: 500\n",
      "Before balancing (brain-tumor-mri): Yes: 5023, No: 2000\n",
      "After balancing (brain-tumor-mri): Yes: 2000, No: 2000\n"
     ]
    }
   ],
   "source": [
    "# Define the base source and destination paths\n",
    "base_data_path = \"data\"\n",
    "new_structure_path = \"data_restructured\"\n",
    "\n",
    "# List of datasets to process\n",
    "datasets = [\"brain-tumor-classification\", \"brain-tumor-mri\"]\n",
    "\n",
    "# Define tumor types for each dataset\n",
    "dataset_tumor_types = {\n",
    "    \"brain-tumor-classification\": {\n",
    "        \"tumor_types\": [\"glioma_tumor\", \"meningioma_tumor\", \"pituitary_tumor\"],\n",
    "        \"no_tumor\": \"no_tumor\"\n",
    "    },\n",
    "    \"brain-tumor-mri\": {\n",
    "        \"tumor_types\": [\"glioma\", \"meningioma\", \"pituitary\"],\n",
    "        \"no_tumor\": \"notumor\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Create dataset-specific restructured path\n",
    "    dataset_restructured_path = os.path.join(new_structure_path, dataset)\n",
    "    os.makedirs(dataset_restructured_path, exist_ok=True)\n",
    "    os.makedirs(os.path.join(dataset_restructured_path, \"yes\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dataset_restructured_path, \"no\"), exist_ok=True)\n",
    "\n",
    "    # Get the tumor types and no_tumor name for the current dataset\n",
    "    tumor_types = dataset_tumor_types[dataset][\"tumor_types\"]\n",
    "    notumor_type = dataset_tumor_types[dataset][\"no_tumor\"]\n",
    "\n",
    "    # Copy tumor data into 'yes' (flattened) for the current dataset\n",
    "    for tumor_type in tumor_types:\n",
    "        for dataset_type in [\"Testing\", \"Training\"]:\n",
    "            source_dir = os.path.join(base_data_path, dataset, dataset_type, tumor_type)\n",
    "            if os.path.exists(source_dir):\n",
    "                # Copy all files directly to 'yes'\n",
    "                for item in os.listdir(source_dir):\n",
    "                    source_item = os.path.join(source_dir, item)\n",
    "                    if os.path.isfile(source_item):\n",
    "                        destination_item = os.path.join(dataset_restructured_path, \"yes\", item)\n",
    "                        # Avoid overwriting by adding a number if file exists\n",
    "                        base_name, extension = os.path.splitext(item)\n",
    "                        counter = 1\n",
    "                        while os.path.exists(destination_item):\n",
    "                            new_name = f\"{base_name}_{counter}{extension}\"\n",
    "                            destination_item = os.path.join(dataset_restructured_path, \"yes\", new_name)\n",
    "                            counter += 1\n",
    "                        shutil.copy2(source_item, destination_item)\n",
    "\n",
    "    # Copy notumor data into 'no' (flattened) for the current dataset\n",
    "    for dataset_type in [\"Testing\", \"Training\"]:\n",
    "        source_dir = os.path.join(base_data_path, dataset, dataset_type, notumor_type)\n",
    "        if os.path.exists(source_dir):\n",
    "            # Copy all files directly to 'no'\n",
    "            for item in os.listdir(source_dir):\n",
    "                source_item = os.path.join(source_dir, item)\n",
    "                if os.path.isfile(source_item):\n",
    "                    destination_item = os.path.join(dataset_restructured_path, \"no\", item)\n",
    "                    # Avoid overwriting by adding a number if file exists\n",
    "                    base_name, extension = os.path.splitext(item)\n",
    "                    counter = 1\n",
    "                    while os.path.exists(destination_item):\n",
    "                        new_name = f\"{base_name}_{counter}{extension}\"\n",
    "                        destination_item = os.path.join(dataset_restructured_path, \"no\", new_name)\n",
    "                        counter += 1\n",
    "                    shutil.copy2(source_item, destination_item)\n",
    "\n",
    "    # Balance the dataset\n",
    "    yes_path = os.path.join(dataset_restructured_path, \"yes\")\n",
    "    no_path = os.path.join(dataset_restructured_path, \"no\")\n",
    "\n",
    "    # Count the number of files in each folder for the current dataset\n",
    "    yes_files = [f for f in os.listdir(yes_path) if os.path.isfile(os.path.join(yes_path, f))]\n",
    "    no_files = [f for f in os.listdir(no_path) if os.path.isfile(os.path.join(no_path, f))]\n",
    "\n",
    "    yes_count = len(yes_files)\n",
    "    no_count = len(no_files)\n",
    "\n",
    "    print(f\"Before balancing ({dataset}): Yes: {yes_count}, No: {no_count}\")\n",
    "\n",
    "    # If yes has more samples than no, randomly remove samples from yes\n",
    "    if yes_count > no_count:\n",
    "        # Calculate how many to remove from yes to match no\n",
    "        excess = yes_count - no_count\n",
    "        # Randomly select files to remove\n",
    "        files_to_remove = random.sample(yes_files, excess)\n",
    "        # Remove the selected files\n",
    "        for file in files_to_remove:\n",
    "            os.remove(os.path.join(yes_path, file))\n",
    "\n",
    "    # Recount after balancing\n",
    "    yes_files = [f for f in os.listdir(yes_path) if os.path.isfile(os.path.join(yes_path, f))]\n",
    "    no_files = [f for f in os.listdir(no_path) if os.path.isfile(os.path.join(no_path, f))]\n",
    "\n",
    "    yes_count = len(yes_files)\n",
    "    no_count = len(no_files)\n",
    "\n",
    "    print(f\"After balancing ({dataset}): Yes: {yes_count}, No: {no_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For dataset 4, run the following cell to prepare the data after downloading it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "def process_mat_files(input_base, output_base):\n",
    "    class_names = {1: \"meningioma\", 2: \"glioma\", 3: \"pituitary\"}\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "    for class_name in class_names.values():\n",
    "        os.makedirs(os.path.join(output_base, class_name), exist_ok=True)\n",
    "\n",
    "    curStart = 1\n",
    "    curEnd = 766\n",
    "\n",
    "    mat_files = []\n",
    "\n",
    "    # Process each part\n",
    "    for _ in range(4):\n",
    "        input_folder = os.path.join(input_base, f\"brainTumorDataPublic_{curStart}-{curEnd}\")\n",
    "        mat_files = [f for f in os.listdir(input_folder) if f.endswith('.mat')]\n",
    "        curStart = curEnd + 1\n",
    "        curEnd = min(curEnd + 766, 3064)\n",
    "        \n",
    "        for mat_file in mat_files:\n",
    "                \n",
    "                file_path = os.path.join(input_folder, mat_file)\n",
    "                try:\n",
    "                    with h5py.File(file_path, 'r') as mat_data:\n",
    "                        cjdata = mat_data['cjdata']\n",
    "                        \n",
    "                        im1 = np.array(cjdata['image'], dtype=np.float64).T  # Transpose if needed\n",
    "                        min1, max1 = im1.min(), im1.max()\n",
    "                        im = ((255 / (max1 - min1)) * (im1 - min1)).astype(np.uint8)\n",
    "                        label = int(cjdata['label'][()][0][0])  # Extract label properly\n",
    "\n",
    "                        label_folder = os.path.join(output_base, class_names[label])\n",
    "                        os.makedirs(label_folder, exist_ok=True)\n",
    "\n",
    "                        file_name_base, _ = os.path.splitext(mat_file)\n",
    "                        output_file_path = os.path.join(label_folder, f\"{file_name_base}.jpg\")\n",
    "\n",
    "                        cv2.imwrite(output_file_path, im)  \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
